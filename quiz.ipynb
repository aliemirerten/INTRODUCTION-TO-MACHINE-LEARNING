{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliemirerten/INTRODUCTION-TO-MACHINE-LEARNING/blob/main/quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rGVC0yq_KFGc",
      "metadata": {
        "id": "rGVC0yq_KFGc"
      },
      "source": [
        "# Undergrad-Friendly Data Science Quiz — Questions Only\n",
        "\n",
        "**Instructions:** This notebook contains only the questions (no solutions).  \n",
        "Answer each question in the provided code cells. You can run all questions directly in **Google Colab**.\n",
        "\n",
        "**Distribution:**\n",
        "- 5 Python basics\n",
        "- 2 Descriptive statistics\n",
        "- 2 Math for data science\n",
        "- 2 Linear algebra\n",
        "- 9 Intro ML topics (linear regression, logistic regression, k-means, k-NN, PCA, etc.)\n",
        "\n",
        "_Generated on 2025-11-01 06:39:35 UTC_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66OQYs3KFGf",
      "metadata": {
        "id": "a66OQYs3KFGf"
      },
      "source": [
        "### Setup (run once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WRLbpXqNKFGg",
      "metadata": {
        "id": "WRLbpXqNKFGg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score, silhouette_score\n",
        "from sklearn.datasets import make_classification, load_iris, make_blobs, make_moons, load_wine\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.set_printoptions(suppress=True, precision=4)\n",
        "rng = np.random.default_rng(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dy4qb_jRKFGi",
      "metadata": {
        "id": "dy4qb_jRKFGi"
      },
      "source": [
        "## Part A — Python Programming (5 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BF9C7wMkKFGj",
      "metadata": {
        "id": "BF9C7wMkKFGj"
      },
      "source": [
        "### Q1 — List basics\n",
        "Create a list of integers from 1 to 10. Print the first 3 and the last 2 elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Jn_OxTqKFGk",
      "metadata": {
        "id": "-Jn_OxTqKFGk",
        "outputId": "a9704dbe-4f1e-4a9a-f7d8-38a58c2777c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full list: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "First 3 elements: [1, 2, 3]\n",
            "Last 2 elements: [9, 10]\n"
          ]
        }
      ],
      "source": [
        "numbers = list(range(1, 11))\n",
        "print(\"Full list:\", numbers)\n",
        "\n",
        "print(\"First 3 elements:\", numbers[:3])\n",
        "\n",
        "print(\"Last 2 elements:\", numbers[-2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K8T-vpT3KFGl",
      "metadata": {
        "id": "K8T-vpT3KFGl"
      },
      "source": [
        "### Q2 — Sum of evens\n",
        "Using a loop or list comprehension, compute the sum of even numbers from 1 to 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yIqSiLyOKFGm",
      "metadata": {
        "id": "yIqSiLyOKFGm",
        "outputId": "8c55b3c1-ebfc-4ad2-c374-6746b7b234d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sum of even numbers (1-20) using loop: 110\n"
          ]
        }
      ],
      "source": [
        "even_sum_loop = 0\n",
        "for i in range(1, 21):\n",
        "    if i % 2 == 0:\n",
        "        even_sum_loop += i\n",
        "print(\"Sum of even numbers (1-20) using loop:\", even_sum_loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mvUOlkb1KFGn",
      "metadata": {
        "id": "mvUOlkb1KFGn"
      },
      "source": [
        "### Q3 — Function with default\n",
        "Write `greet(name, lang='EN')` that prints 'Hello, <name>!' if EN and 'Merhaba, <name>!' if TR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ybxhFiM0KFGn",
      "metadata": {
        "id": "ybxhFiM0KFGn",
        "outputId": "8d33ff6e-8e00-4880-e85c-1ab0d7eeacf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, Alice!\n",
            "Merhaba, Ahmet!\n",
            "Hello, Bob!\n"
          ]
        }
      ],
      "source": [
        "def greet(name, lang='EN'):\n",
        "    if lang == 'EN':\n",
        "        print(f'Hello, {name}!')\n",
        "    elif lang == 'TR':\n",
        "        print(f'Merhaba, {name}!')\n",
        "    else:\n",
        "        print(f'Unknown language: {lang}')\n",
        "\n",
        "greet(\"Alice\")\n",
        "greet(\"Ahmet\", \"TR\")\n",
        "greet(\"Bob\", \"EN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GUcurgBdKFGo",
      "metadata": {
        "id": "GUcurgBdKFGo"
      },
      "source": [
        "### Q4 — Dictionary counts\n",
        "Given `text = 'data science is fun and data is useful'`, build a dict mapping each word to its count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LHpK0aQ8KFGp",
      "metadata": {
        "id": "LHpK0aQ8KFGp",
        "outputId": "0b2befb9-7047-4dea-f6a8-9642f743f802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word count {'data': 2, 'science': 1, 'is': 2, 'fun': 1, 'and': 1, 'useful': 1}\n"
          ]
        }
      ],
      "source": [
        "text = 'data science is fun and data is useful'\n",
        "\n",
        "word_count = {}\n",
        "words = text.split()\n",
        "for word in words:\n",
        "    if word in word_count:\n",
        "        word_count[word] += 1\n",
        "    else:\n",
        "        word_count[word] = 1\n",
        "\n",
        "print(\"Word count\", word_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lP1D9K75KFGp",
      "metadata": {
        "id": "lP1D9K75KFGp"
      },
      "source": [
        "### Q5 — Simple NumPy array\n",
        "Create a NumPy array `a = [1,2,3,4]`. Compute elementwise square and the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zq5R6Xp_KFGq",
      "metadata": {
        "id": "zq5R6Xp_KFGq",
        "outputId": "ca0fa89f-8232-4d8c-c0f7-dbdfb2bd01fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original array: [1 2 3 4]\n",
            "Elementwise square: [ 1  4  9 16]\n",
            "Mean of original array: 2.5\n",
            "Mean of squared array: 7.5\n"
          ]
        }
      ],
      "source": [
        "a = np.array([1, 2, 3, 4])\n",
        "print(\"Original array:\", a)\n",
        "\n",
        "a_squared = a ** 2\n",
        "print(\"Elementwise square:\", a_squared)\n",
        "\n",
        "mean_value = np.mean(a)\n",
        "print(\"Mean of original array:\", mean_value)\n",
        "\n",
        "mean_squared = np.mean(a_squared)\n",
        "print(\"Mean of squared array:\", mean_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRLeTySTKFGq",
      "metadata": {
        "id": "qRLeTySTKFGq"
      },
      "source": [
        "## Part B — Descriptive Statistics (2 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QE-h_jj-KFGq",
      "metadata": {
        "id": "QE-h_jj-KFGq"
      },
      "source": [
        "### Q6 — Mean/Median/Mode/Std\n",
        "For `data = [1,2,2,3,4,4,4,5]`, compute mean, median, mode, and population std."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-3EHeVltKFGr",
      "metadata": {
        "id": "-3EHeVltKFGr",
        "outputId": "43c1282d-9d1a-4c46-a9fc-b6eba654a36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: [1, 2, 2, 3, 4, 4, 4, 5]\n",
            "Mean: 3.125\n",
            "Median: 3.5\n",
            "Mode: 4 (appears 3 times)\n",
            "Population standard deviation: 1.2686114456365274\n",
            "Sample standard deviation: 1.3562026818605375\n"
          ]
        }
      ],
      "source": [
        "data = [1, 2, 2, 3, 4, 4, 4, 5]\n",
        "print(\"Data:\", data)\n",
        "\n",
        "mean = np.mean(data)\n",
        "print(f\"Mean: {mean}\")\n",
        "\n",
        "median = np.median(data)\n",
        "print(f\"Median: {median}\")\n",
        "\n",
        "mode_result = stats.mode(data, keepdims=True)\n",
        "mode_value = mode_result.mode[0]\n",
        "mode_count = mode_result.count[0]\n",
        "print(f\"Mode: {mode_value} (appears {mode_count} times)\")\n",
        "\n",
        "pop_std = np.std(data, ddof=0)\n",
        "print(f\"Population standard deviation: {pop_std}\")\n",
        "\n",
        "sample_std = np.std(data, ddof=1)\n",
        "print(f\"Sample standard deviation: {sample_std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y3Hp7ATnKFGr",
      "metadata": {
        "id": "y3Hp7ATnKFGr"
      },
      "source": [
        "### Q7 — Quartiles & simple outlier rule\n",
        "For `x = [10,11,12,13,14,50]`, find Q1, Q3, and IQR. Mark values greater than `Q3 + 1.5×IQR` as outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Du66Ur2VKFGs",
      "metadata": {
        "id": "Du66Ur2VKFGs",
        "outputId": "90636003-56e8-4694-a084-7662f82140e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: [10, 11, 12, 13, 14, 50]\n",
            "Q1 (25th percentile): 11.25\n",
            "Q3 (75th percentile): 13.75\n",
            "IQR (Interquartile Range): 2.5\n",
            "Outlier threshold (Q3 + 1.5×IQR): 17.5\n",
            "Values greater than 17.5: [50]\n",
            "Normal values: [10, 11, 12, 13, 14]\n",
            "Outliers detected: [50]\n"
          ]
        }
      ],
      "source": [
        "x = [10, 11, 12, 13, 14, 50]\n",
        "print(\"Data:\", x)\n",
        "\n",
        "Q1 = np.percentile(x, 25)\n",
        "Q3 = np.percentile(x, 75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(f\"Q1 (25th percentile): {Q1}\")\n",
        "print(f\"Q3 (75th percentile): {Q3}\")\n",
        "print(f\"IQR (Interquartile Range): {IQR}\")\n",
        "\n",
        "outlier_threshold = Q3 + 1.5 * IQR\n",
        "print(f\"Outlier threshold (Q3 + 1.5×IQR): {outlier_threshold}\")\n",
        "\n",
        "outliers = []\n",
        "normal_values = []\n",
        "\n",
        "for value in x:\n",
        "    if value > outlier_threshold:\n",
        "        outliers.append(value)\n",
        "    else:\n",
        "        normal_values.append(value)\n",
        "\n",
        "print(f\"Values greater than {outlier_threshold}: {outliers}\")\n",
        "print(f\"Normal values: {normal_values}\")\n",
        "\n",
        "if outliers:\n",
        "    print(f\"Outliers detected: {outliers}\")\n",
        "else:\n",
        "    print(\"No outliers detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XyVnq0SKKFGs",
      "metadata": {
        "id": "XyVnq0SKKFGs"
      },
      "source": [
        "## Part C — Math for Data Science (2 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CCUYogwJKFGs",
      "metadata": {
        "id": "CCUYogwJKFGs"
      },
      "source": [
        "### Q8 — Simple probability\n",
        "A fair six-sided die is rolled once. Compute the probability of getting an even number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rFfo0lJYKFGt",
      "metadata": {
        "id": "rFfo0lJYKFGt",
        "outputId": "6de263fc-b577-42eb-daa8-4a48f1d0a199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total possible outcomes: 6\n",
            "Even numbers on die: [2, 4, 6]\n",
            "Number of favorable outcomes (even numbers): 3\n",
            "Probability of getting an even number: 0.5\n",
            "Probability as fraction: 3/6\n",
            "Probability as percentage: 50.0%\n"
          ]
        }
      ],
      "source": [
        "total_outcomes = 6\n",
        "print(f\"Total possible outcomes: {total_outcomes}\")\n",
        "\n",
        "even_numbers = [2, 4, 6]\n",
        "favorable_outcomes = len(even_numbers)\n",
        "print(f\"Even numbers on die: {even_numbers}\")\n",
        "print(f\"Number of favorable outcomes (even numbers): {favorable_outcomes}\")\n",
        "\n",
        "probability = favorable_outcomes / total_outcomes\n",
        "print(f\"Probability of getting an even number: {probability}\")\n",
        "\n",
        "print(f\"Probability as fraction: {favorable_outcomes}/{total_outcomes}\")\n",
        "\n",
        "percentage = probability * 100\n",
        "print(f\"Probability as percentage: {percentage}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SzpW4GjWKFGt",
      "metadata": {
        "id": "SzpW4GjWKFGt"
      },
      "source": [
        "### Q9 — Z-score\n",
        "Given `x = 70`, mean = 60, std = 10, compute the z-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "au7K_mgtKFGt",
      "metadata": {
        "id": "au7K_mgtKFGt",
        "outputId": "946cfaa8-461e-440b-edd4-03340cfff75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 70\n",
            "Mean: 60\n",
            "Standard deviation: 10\n",
            "Z-score calculation: (70 - 60) / 10 = 1.0\n",
            "Interpretation: The value 70 is 1.0 standard deviations ABOVE the mean.\n",
            "Verification using scipy: nan\n"
          ]
        }
      ],
      "source": [
        "x = 70\n",
        "mean = 60\n",
        "std = 10\n",
        "\n",
        "print(f\"Value: {x}\")\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard deviation: {std}\")\n",
        "\n",
        "z_score = (x - mean) / std\n",
        "\n",
        "print(f\"Z-score calculation: ({x} - {mean}) / {std} = {z_score}\")\n",
        "\n",
        "if z_score > 0:\n",
        "    print(f\"Interpretation: The value {x} is {z_score} standard deviations ABOVE the mean.\")\n",
        "elif z_score < 0:\n",
        "    print(f\"Interpretation: The value {x} is {abs(z_score)} standard deviations BELOW the mean.\")\n",
        "else:\n",
        "    print(f\"Interpretation: The value {x} is exactly at the mean.\")\n",
        "\n",
        "z_score_scipy = stats.zscore([x], ddof=0)[0] if std != 0 else 0\n",
        "print(f\"Verification using scipy: {z_score_scipy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3j1TS2exKFGu",
      "metadata": {
        "id": "3j1TS2exKFGu"
      },
      "source": [
        "## Part D — Basic Linear Algebra (2 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PGf-UyyuKFGu",
      "metadata": {
        "id": "PGf-UyyuKFGu"
      },
      "source": [
        "### Q10 — Dot product & norm\n",
        "Let `u = [3,4]`, `v = [1,2]`. Compute the dot product `u·v` and the Euclidean norm of `u`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pyEiNuBOKFGu",
      "metadata": {
        "id": "pyEiNuBOKFGu",
        "outputId": "2fd56558-97df-4596-8dbc-bf97ad7260c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector u: [3 4]\n",
            "Vector v: [1 2]\n",
            "Dot product u·v: 11\n",
            "Euclidean norm of u: 5.0\n"
          ]
        }
      ],
      "source": [
        "u = np.array([3, 4])\n",
        "v = np.array([1, 2])\n",
        "\n",
        "print(f\"Vector u: {u}\")\n",
        "print(f\"Vector v: {v}\")\n",
        "\n",
        "dot_product = np.dot(u, v)\n",
        "print(f\"Dot product u·v: {dot_product}\")\n",
        "\n",
        "norm_u = np.linalg.norm(u)\n",
        "print(f\"Euclidean norm of u: {norm_u}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L2FBu6E8KFGv",
      "metadata": {
        "id": "L2FBu6E8KFGv"
      },
      "source": [
        "### Q11 — Solve a 2×2 system\n",
        "Solve `A x = b` for `x` where `A = [[2,1],[1,3]]` and `b = [5,7]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HT-YJ1hCKFGv",
      "metadata": {
        "id": "HT-YJ1hCKFGv",
        "outputId": "8e212cfb-2340-4aef-9aa3-f48c4d03cc16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix A:\n",
            "[[2 1]\n",
            " [1 3]]\n",
            "Vector b: [5 7]\n",
            "\n",
            "Solution:\n",
            "x = [1.6 1.8]\n",
            "x₁ = 1.6, x₂ = 1.8\n",
            "\n",
            "Determinant of A: 5.000000000000001\n",
            "Solution using Cramer's rule:\n",
            "x₁ = 1.5999999999999988, x₂ = 1.8\n",
            "\n",
            "Verification (A @ x should equal b):\n",
            "A @ x = [5. 7.]\n",
            "b     = [5 7]\n",
            "Equal? True\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[2, 1],\n",
        "              [1, 3]])\n",
        "b = np.array([5, 7])\n",
        "\n",
        "print(\"Matrix A:\")\n",
        "print(A)\n",
        "print(f\"Vector b: {b}\")\n",
        "print()\n",
        "\n",
        "x = np.linalg.solve(A, b)\n",
        "print(\"Solution:\")\n",
        "print(f\"x = {x}\")\n",
        "print(f\"x₁ = {x[0]}, x₂ = {x[1]}\")\n",
        "print()\n",
        "\n",
        "det_A = np.linalg.det(A)\n",
        "print(f\"Determinant of A: {det_A}\")\n",
        "\n",
        "x1_cramer = np.linalg.det([[b[0], A[0,1]], [b[1], A[1,1]]]) / det_A\n",
        "x2_cramer = np.linalg.det([[A[0,0], b[0]], [A[1,0], b[1]]]) / det_A\n",
        "\n",
        "print(\"Solution using Cramer's rule:\")\n",
        "print(f\"x₁ = {x1_cramer}, x₂ = {x2_cramer}\")\n",
        "print()\n",
        "\n",
        "verification = A @ x\n",
        "print(\"Verification (A @ x should equal b):\")\n",
        "print(f\"A @ x = {verification}\")\n",
        "print(f\"b     = {b}\")\n",
        "print(f\"Equal? {np.allclose(verification, b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VZSHRd72KFGv",
      "metadata": {
        "id": "VZSHRd72KFGv"
      },
      "source": [
        "## Part E — Intro ML (9 questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MH4a3ntTKFGw",
      "metadata": {
        "id": "MH4a3ntTKFGw"
      },
      "source": [
        "### Q12 — Linear regression (tiny)\n",
        "Generate 50 points: `y = 2x + 1 + noise` with `x ~ U[0,5]`, noise ~ N(0,0.5). Fit `LinearRegression` and print slope, intercept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D743C0I5KFGw",
      "metadata": {
        "id": "D743C0I5KFGw",
        "outputId": "e1bedae2-e2fa-4d01-ea6d-ee1e32e0cecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 50 data points\n",
            "x range: [0.10, 4.85]\n",
            "y range: [1.14, 11.27]\n",
            "True relationship: y = 2x + 1 + noise\n",
            "\n",
            "Linear Regression Results:\n",
            "Fitted equation: y = 1.9777x + 1.0483\n",
            "Slope (coefficient): 1.9777 (true value: 2.0)\n",
            "Intercept: 1.0483 (true value: 1.0)\n",
            "\n",
            "Slope error: 0.0223\n",
            "Intercept error: 0.0483\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 50\n",
        "\n",
        "x = np.random.uniform(0, 5, n_samples)\n",
        "\n",
        "noise = np.random.normal(0, 0.5, n_samples)\n",
        "\n",
        "y = 2 * x + 1 + noise\n",
        "\n",
        "print(f\"Generated {n_samples} data points\")\n",
        "print(f\"x range: [{x.min():.2f}, {x.max():.2f}]\")\n",
        "print(f\"y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
        "print(f\"True relationship: y = 2x + 1 + noise\")\n",
        "print()\n",
        "\n",
        "X = x.reshape(-1, 1)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "print(\"Linear Regression Results:\")\n",
        "print(f\"Fitted equation: y = {slope:.4f}x + {intercept:.4f}\")\n",
        "print(f\"Slope (coefficient): {slope:.4f} (true value: 2.0)\")\n",
        "print(f\"Intercept: {intercept:.4f} (true value: 1.0)\")\n",
        "print()\n",
        "\n",
        "slope_error = abs(slope - 2.0)\n",
        "intercept_error = abs(intercept - 1.0)\n",
        "print(f\"Slope error: {slope_error:.4f}\")\n",
        "print(f\"Intercept error: {intercept_error:.4f}\")\n",
        "\n",
        "globals()['X_q12'] = X\n",
        "globals()['y_q12'] = y\n",
        "globals()['model_q12'] = model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_la2oaLsKFGx",
      "metadata": {
        "id": "_la2oaLsKFGx"
      },
      "source": [
        "### Q13 — Linear regression R²\n",
        "Using the model from Q12, compute and print the R² score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1m1FW8KKFGx",
      "metadata": {
        "id": "S1m1FW8KKFGx",
        "outputId": "00f2fe13-11ea-476f-e386-7aaf2a4f9760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using data and model from Q12...\n",
            "Data shape: X=(50, 1), y=(50,)\n",
            "\n",
            "R² score: 0.9749\n",
            "\n",
            "R² Interpretation:\n",
            "R² = 0.9749 means 97.49% of the variance in y is explained by x\n",
            "This indicates a very good fit!\n",
            "\n",
            "Additional metrics:\n",
            "Mean Squared Error (MSE): 28.2985\n",
            "Root Mean Squared Error (RMSE): 5.3196\n"
          ]
        }
      ],
      "source": [
        "X = globals().get('X_q12')\n",
        "y = globals().get('y_q12')\n",
        "model = globals().get('model_q12')\n",
        "\n",
        "if X is None or y is None or model is None:\n",
        "    print(\"Error: Please run Q12 first to generate the data and model!\")\n",
        "else:\n",
        "    print(\"Using data and model from Q12...\")\n",
        "    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
        "    print()\n",
        "\n",
        "    r2_score_method1 = model.score(X, y)\n",
        "    print(f\"R² score: {r2_score_method1:.4f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"R² Interpretation:\")\n",
        "    print(f\"R² = {r2_score_method1:.4f} means {r2_score_method1*100:.2f}% of the variance in y is explained by x\")\n",
        "\n",
        "    if r2_score_method1 > 0.8:\n",
        "        print(\"This indicates a very good fit!\")\n",
        "    elif r2_score_method1 > 0.6:\n",
        "        print(\"This indicates a good fit.\")\n",
        "    elif r2_score_method1 > 0.4:\n",
        "        print(\"This indicates a moderate fit.\")\n",
        "    else:\n",
        "        print(\"This indicates a poor fit.\")\n",
        "\n",
        "    print()\n",
        "    print(\"Additional metrics:\")\n",
        "    print(f\"Mean Squared Error (MSE): {np.mean((y - r2_score_method1)**2):.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {np.sqrt(np.mean((y - r2_score_method1)**2)):.4f}\")\n",
        "\n",
        "    globals()['r2_score_q13'] = r2_score_method1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q4seSconKFGx",
      "metadata": {
        "id": "Q4seSconKFGx"
      },
      "source": [
        "### Q14 — Logistic regression (easy)\n",
        "Create a simple binary dataset with `make_moons(n_samples=400, noise=0.2)`. Train/test split 80/20. Fit `LogisticRegression` and print accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WVfUm-VFKFGy",
      "metadata": {
        "id": "WVfUm-VFKFGy",
        "outputId": "ad307789-586d-4c00-e1a1-bc7869c8794a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: X=(400, 2), y=(400,)\n",
            "Number of samples: 400\n",
            "Number of features: 2\n",
            "Classes: [0 1]\n",
            "Class distribution: [200 200]\n",
            "\n",
            "Training set: X_train=(320, 2), y_train=(320,)\n",
            "Test set: X_test=(80, 2), y_test=(80,)\n",
            "Train class distribution: [160 160]\n",
            "Test class distribution: [40 40]\n",
            "\n",
            "Logistic Regression Results:\n",
            "Test Accuracy: 0.9375 (93.75%)\n",
            "\n",
            "Additional Classification Metrics:\n",
            "Precision: 0.9268\n",
            "Recall: 0.9500\n",
            "F1-score: 0.9383\n",
            "\n",
            "Sample predictions (first 5 test samples):\n",
            "Sample 1: Actual=0, Predicted=1, Prob(0)=0.484, Prob(1)=0.516\n",
            "Sample 2: Actual=1, Predicted=1, Prob(0)=0.281, Prob(1)=0.719\n",
            "Sample 3: Actual=1, Predicted=1, Prob(0)=0.036, Prob(1)=0.964\n",
            "Sample 4: Actual=0, Predicted=0, Prob(0)=0.965, Prob(1)=0.035\n",
            "Sample 5: Actual=0, Predicted=0, Prob(0)=0.746, Prob(1)=0.254\n"
          ]
        }
      ],
      "source": [
        "X, y = make_moons(n_samples=400, noise=0.2, random_state=42)\n",
        "\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "print()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "print()\n",
        "\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Additional Classification Metrics:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print()\n",
        "\n",
        "print(\"Sample predictions (first 5 test samples):\")\n",
        "for i in range(min(5, len(y_test))):\n",
        "    actual = y_test[i]\n",
        "    predicted = y_pred[i]\n",
        "    prob_class_0 = y_pred_proba[i][0]\n",
        "    prob_class_1 = y_pred_proba[i][1]\n",
        "    print(f\"Sample {i+1}: Actual={actual}, Predicted={predicted}, Prob(0)={prob_class_0:.3f}, Prob(1)={prob_class_1:.3f}\")\n",
        "\n",
        "globals()['X_test_q14'] = X_test\n",
        "globals()['y_test_q14'] = y_test\n",
        "globals()['model_q14'] = model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HppGk5ABKFGy",
      "metadata": {
        "id": "HppGk5ABKFGy"
      },
      "source": [
        "### Q15 — k-NN (Iris)\n",
        "Load Iris. Split 80/20. Fit `KNeighborsClassifier(n_neighbors=3)` and print accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vVJs2tkoKFGz",
      "metadata": {
        "id": "vVJs2tkoKFGz",
        "outputId": "e86c8155-b502-4cae-dab9-cbb79cbd227c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris Dataset Information:\n",
            "Dataset shape: X=(150, 4), y=(150,)\n",
            "Number of samples: 150\n",
            "Number of features: 4\n",
            "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Classes: ['setosa' 'versicolor' 'virginica']\n",
            "Class labels: [0 1 2]\n",
            "Class distribution: [50 50 50]\n",
            "\n",
            "Training set: X_train=(120, 4), y_train=(120,)\n",
            "Test set: X_test=(30, 4), y_test=(30,)\n",
            "Train class distribution: [40 40 40]\n",
            "Test class distribution: [10 10 10]\n",
            "\n",
            "k-NN Classification Results:\n",
            "k (number of neighbors): 3\n",
            "Test Accuracy: 1.0000 (100.00%)\n",
            "\n",
            "Sample predictions (first 10 test samples):\n",
            "Sample | Actual | Predicted | Actual Name      | Predicted Name\n",
            "-----------------------------------------------------------------\n",
            "     1 |      0 |         0 | setosa          | setosa          ✓\n",
            "     2 |      2 |         2 | virginica       | virginica       ✓\n",
            "     3 |      1 |         1 | versicolor      | versicolor      ✓\n",
            "     4 |      1 |         1 | versicolor      | versicolor      ✓\n",
            "     5 |      0 |         0 | setosa          | setosa          ✓\n",
            "     6 |      1 |         1 | versicolor      | versicolor      ✓\n",
            "     7 |      0 |         0 | setosa          | setosa          ✓\n",
            "     8 |      0 |         0 | setosa          | setosa          ✓\n",
            "     9 |      2 |         2 | virginica       | virginica       ✓\n",
            "    10 |      1 |         1 | versicolor      | versicolor      ✓\n",
            "\n",
            "Correct predictions in sample: 10/10\n",
            "\n",
            "Additional k-NN Information:\n",
            "Distance metric: minkowski\n",
            "Algorithm used: auto\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(\"Iris Dataset Information:\")\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Feature names: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Class labels: {np.unique(y)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "print()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "print()\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"k-NN Classification Results:\")\n",
        "print(f\"k (number of neighbors): 3\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"Sample predictions (first 10 test samples):\")\n",
        "print(\"Sample | Actual | Predicted | Actual Name      | Predicted Name\")\n",
        "print(\"-\" * 65)\n",
        "for i in range(min(10, len(y_test))):\n",
        "    actual = y_test[i]\n",
        "    predicted = y_pred[i]\n",
        "    actual_name = iris.target_names[actual]\n",
        "    predicted_name = iris.target_names[predicted]\n",
        "    match = \"✓\" if actual == predicted else \"✗\"\n",
        "    print(f\"{i+1:6d} | {actual:6d} | {predicted:9d} | {actual_name:15s} | {predicted_name:15s} {match}\")\n",
        "\n",
        "print()\n",
        "print(f\"Correct predictions in sample: {np.sum(y_pred[:10] == y_test[:10])}/10\")\n",
        "\n",
        "print(\"\\nAdditional k-NN Information:\")\n",
        "print(f\"Distance metric: {knn_model.metric}\")\n",
        "print(f\"Algorithm used: {knn_model.algorithm}\")\n",
        "\n",
        "globals()['X_test_q15'] = X_test\n",
        "globals()['y_test_q15'] = y_test\n",
        "globals()['y_pred_q15'] = y_pred\n",
        "globals()['knn_model_q15'] = knn_model\n",
        "globals()['iris_target_names'] = iris.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8AmdhyKFGz",
      "metadata": {
        "id": "fb8AmdhyKFGz"
      },
      "source": [
        "### Q16 — Confusion matrix (Iris + k-NN)\n",
        "Using the model from Q15, print the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VS_c5ElKFG8",
      "metadata": {
        "id": "5VS_c5ElKFG8",
        "outputId": "785e5e23-9e4c-49f8-b38c-613e7000c0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using k-NN predictions from Q15...\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0 10]]\n",
            "\n",
            "Confusion Matrix with Class Names:\n",
            "                  setosa  versicolor   virginica\n",
            "      setosa          10           0           0\n",
            "  versicolor           0          10           0\n",
            "   virginica           0           0          10\n",
            "\n",
            "Confusion Matrix Analysis:\n",
            "--------------------------------------------------\n",
            "Total test samples: 30\n",
            "Correct predictions: 30\n",
            "Overall accuracy: 1.0000 (100.00%)\n",
            "\n",
            "Per-Class Analysis:\n",
            "       Class  Precision   Recall   F1-Score  Support\n",
            "----------------------------------------------------------\n",
            "      setosa     1.0000   1.0000     1.0000       10\n",
            "  versicolor     1.0000   1.0000     1.0000       10\n",
            "   virginica     1.0000   1.0000     1.0000       10\n",
            "\n",
            "Misclassification Analysis:\n",
            "  No misclassifications! Perfect prediction.\n",
            "\n",
            "Confusion Matrix (Percentages by Row):\n",
            "                  setosa  versicolor   virginica\n",
            "      setosa      100.0%        0.0%        0.0%\n",
            "  versicolor        0.0%      100.0%        0.0%\n",
            "   virginica        0.0%        0.0%      100.0%\n",
            "\n",
            "Note: Each row shows the percentage distribution of actual class predictions.\n"
          ]
        }
      ],
      "source": [
        "y_test = globals().get('y_test_q15')\n",
        "y_pred = globals().get('y_pred_q15')\n",
        "target_names = globals().get('iris_target_names')\n",
        "\n",
        "if y_test is None or y_pred is None:\n",
        "    print(\"Error: Please run Q15 first to generate the k-NN model and predictions!\")\n",
        "else:\n",
        "    print(\"Using k-NN predictions from Q15...\")\n",
        "    print()\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print()\n",
        "\n",
        "    print(\"Confusion Matrix with Class Names:\")\n",
        "    print(f\"{'':>12}\", end=\"\")\n",
        "    for name in target_names:\n",
        "        print(f\"{name:>12}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "    for i, name in enumerate(target_names):\n",
        "        print(f\"{name:>12}\", end=\"\")\n",
        "        for j in range(len(target_names)):\n",
        "            print(f\"{cm[i,j]:>12}\", end=\"\")\n",
        "        print()\n",
        "    print()\n",
        "\n",
        "    print(\"Confusion Matrix Analysis:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    total_samples = np.sum(cm)\n",
        "    correct_predictions = np.trace(cm)\n",
        "    overall_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Total test samples: {total_samples}\")\n",
        "    print(f\"Correct predictions: {correct_predictions}\")\n",
        "    print(f\"Overall accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "    print(\"Per-Class Analysis:\")\n",
        "    print(f\"{'Class':>12} {'Precision':>10} {'Recall':>8} {'F1-Score':>10} {'Support':>8}\")\n",
        "    print(\"-\" * 58)\n",
        "\n",
        "    for i, class_name in enumerate(target_names):\n",
        "        tp = cm[i, i]\n",
        "        fp = np.sum(cm[:, i]) - tp\n",
        "        fn = np.sum(cm[i, :]) - tp\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        support = np.sum(cm[i, :])\n",
        "\n",
        "        print(f\"{class_name:>12} {precision:>10.4f} {recall:>8.4f} {f1:>10.4f} {support:>8}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    print(\"Misclassification Analysis:\")\n",
        "    misclassified = 0\n",
        "    for i in range(len(target_names)):\n",
        "        for j in range(len(target_names)):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                actual_class = target_names[i]\n",
        "                predicted_class = target_names[j]\n",
        "                count = cm[i, j]\n",
        "                misclassified += count\n",
        "                print(f\"  {actual_class} → {predicted_class}: {count} sample(s)\")\n",
        "\n",
        "    if misclassified == 0:\n",
        "        print(\"  No misclassifications! Perfect prediction.\")\n",
        "    else:\n",
        "        print(f\"  Total misclassified: {misclassified}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    print(\"Confusion Matrix (Percentages by Row):\")\n",
        "    print(f\"{'':>12}\", end=\"\")\n",
        "    for name in target_names:\n",
        "        print(f\"{name:>12}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "    for i, name in enumerate(target_names):\n",
        "        print(f\"{name:>12}\", end=\"\")\n",
        "        for j in range(len(target_names)):\n",
        "            print(f\"{cm_percent[i,j]:>11.1f}%\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    print(\"\\nNote: Each row shows the percentage distribution of actual class predictions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qud_z6UOKFG8",
      "metadata": {
        "id": "Qud_z6UOKFG8"
      },
      "source": [
        "### Q17 — Decision Tree (easy)\n",
        "On Iris, fit a small `DecisionTreeClassifier(max_depth=3)` and print test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MQqjtPYVKFG9",
      "metadata": {
        "id": "MQqjtPYVKFG9",
        "outputId": "301f24ba-a912-4d7f-fa83-17f77889cc44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree on Iris Dataset:\n",
            "Dataset shape: X=(150, 4), y=(150,)\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Classes: ['setosa' 'versicolor' 'virginica']\n",
            "\n",
            "Training set: X_train=(120, 4), y_train=(120,)\n",
            "Test set: X_test=(30, 4), y_test=(30,)\n",
            "\n",
            "Decision Tree Results:\n",
            "Max depth: 3\n",
            "Criterion: gini\n",
            "Test Accuracy: 0.9667 (96.67%)\n",
            "\n",
            "Decision Tree Structure:\n",
            "Number of nodes: 9\n",
            "Number of leaves: 5\n",
            "Actual tree depth: 3\n",
            "\n",
            "Feature Importance:\n",
            "sepal length (cm)   : 0.0000\n",
            "sepal width (cm)    : 0.0000\n",
            "petal length (cm)   : 0.5791\n",
            "petal width (cm)    : 0.4209\n",
            "\n",
            "Most important feature: petal length (cm) (importance: 0.5791)\n",
            "\n",
            "Sample predictions (first 8 test samples):\n",
            "Sample | Actual | Predicted | Actual Name      | Predicted Name     | Match\n",
            "---------------------------------------------------------------------------\n",
            "     1 |      0 |         0 | setosa          | setosa           | ✓\n",
            "     2 |      2 |         2 | virginica       | virginica        | ✓\n",
            "     3 |      1 |         1 | versicolor      | versicolor       | ✓\n",
            "     4 |      1 |         1 | versicolor      | versicolor       | ✓\n",
            "     5 |      0 |         0 | setosa          | setosa           | ✓\n",
            "     6 |      1 |         1 | versicolor      | versicolor       | ✓\n",
            "     7 |      0 |         0 | setosa          | setosa           | ✓\n",
            "     8 |      0 |         0 | setosa          | setosa           | ✓\n",
            "\n",
            "Correct predictions in sample: 8/8\n",
            "\n",
            "Comparison with k-NN (Q15):\n",
            "Decision Tree accuracy: 0.9667\n",
            "k-NN accuracy:          1.0000\n",
            "k-NN performs better!\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(\"Decision Tree on Iris Dataset:\")\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Features: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "print()\n",
        "\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    max_depth=3,\n",
        "    random_state=42,\n",
        "    criterion='gini'\n",
        ")\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Decision Tree Results:\")\n",
        "print(f\"Max depth: {dt_model.max_depth}\")\n",
        "print(f\"Criterion: {dt_model.criterion}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"Decision Tree Structure:\")\n",
        "print(f\"Number of nodes: {dt_model.tree_.node_count}\")\n",
        "print(f\"Number of leaves: {dt_model.tree_.n_leaves}\")\n",
        "print(f\"Actual tree depth: {dt_model.tree_.max_depth}\")\n",
        "print()\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "feature_importance = dt_model.feature_importances_\n",
        "for i, (feature_name, importance) in enumerate(zip(iris.feature_names, feature_importance)):\n",
        "    print(f\"{feature_name:20s}: {importance:.4f}\")\n",
        "print()\n",
        "\n",
        "most_important_idx = np.argmax(feature_importance)\n",
        "most_important_feature = iris.feature_names[most_important_idx]\n",
        "print(f\"Most important feature: {most_important_feature} (importance: {feature_importance[most_important_idx]:.4f})\")\n",
        "print()\n",
        "\n",
        "print(\"Sample predictions (first 8 test samples):\")\n",
        "print(\"Sample | Actual | Predicted | Actual Name      | Predicted Name     | Match\")\n",
        "print(\"-\" * 75)\n",
        "for i in range(min(8, len(y_test))):\n",
        "    actual = y_test[i]\n",
        "    predicted = y_pred[i]\n",
        "    actual_name = iris.target_names[actual]\n",
        "    predicted_name = iris.target_names[predicted]\n",
        "    match = \"✓\" if actual == predicted else \"✗\"\n",
        "    print(f\"{i+1:6d} | {actual:6d} | {predicted:9d} | {actual_name:15s} | {predicted_name:15s}  | {match}\")\n",
        "\n",
        "print()\n",
        "print(f\"Correct predictions in sample: {np.sum(y_pred[:8] == y_test[:8])}/8\")\n",
        "\n",
        "knn_accuracy = None\n",
        "if 'y_pred_q15' in globals() and 'y_test_q15' in globals():\n",
        "    y_test_knn = globals()['y_test_q15']\n",
        "    y_pred_knn = globals()['y_pred_q15']\n",
        "    if np.array_equal(y_test, y_test_knn):  # Same test set\n",
        "        knn_accuracy = accuracy_score(y_test_knn, y_pred_knn)\n",
        "        print(f\"\\nComparison with k-NN (Q15):\")\n",
        "        print(f\"Decision Tree accuracy: {accuracy:.4f}\")\n",
        "        print(f\"k-NN accuracy:          {knn_accuracy:.4f}\")\n",
        "        if accuracy > knn_accuracy:\n",
        "            print(\"Decision Tree performs better!\")\n",
        "        elif accuracy < knn_accuracy:\n",
        "            print(\"k-NN performs better!\")\n",
        "        else:\n",
        "            print(\"Both models perform equally!\")\n",
        "\n",
        "globals()['dt_model_q17'] = dt_model\n",
        "globals()['dt_accuracy_q17'] = accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EdcxYtSvKFG9",
      "metadata": {
        "id": "EdcxYtSvKFG9"
      },
      "source": [
        "### Q18 — Naive Bayes (Iris)\n",
        "Train `GaussianNB` on Iris with an 80/20 split. Print accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptIGi35CKFG9",
      "metadata": {
        "id": "ptIGi35CKFG9",
        "outputId": "3e3e514c-a548-4c38-ec53-a6c7aec4b1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes on Iris Dataset:\n",
            "Dataset shape: X=(150, 4), y=(150,)\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Classes: ['setosa' 'versicolor' 'virginica']\n",
            "Class distribution: [50 50 50]\n",
            "\n",
            "Training set: X_train=(120, 4), y_train=(120,)\n",
            "Test set: X_test=(30, 4), y_test=(30,)\n",
            "Train class distribution: [40 40 40]\n",
            "Test class distribution: [10 10 10]\n",
            "\n",
            "Gaussian Naive Bayes Results:\n",
            "Test Accuracy: 0.9667 (96.67%)\n",
            "\n",
            "Model Parameters:\n",
            "Classes: [0 1 2]\n",
            "Number of training samples per class: [40. 40. 40.]\n",
            "\n",
            "Feature Statistics (means for each class):\n",
            "Feature                        setosa  versicolor   virginica\n",
            "sepal length (cm)                4.99        5.93        6.61\n",
            "sepal width (cm)                 3.41        2.75        2.98\n",
            "petal length (cm)                1.48        4.25        5.58\n",
            "petal width (cm)                 0.25        1.32        2.04\n",
            "\n",
            "Sample predictions (first 8 test samples):\n",
            "Sample | Actual | Predicted | Actual Name      | Predicted Name     | Max Prob | Match\n",
            "-------------------------------------------------------------------------------------\n",
            "     1 |      0 |         0 | setosa          | setosa           |    1.000 | ✓\n",
            "     2 |      2 |         2 | virginica       | virginica        |    0.912 | ✓\n",
            "     3 |      1 |         1 | versicolor      | versicolor       |    1.000 | ✓\n",
            "     4 |      1 |         1 | versicolor      | versicolor       |    1.000 | ✓\n",
            "     5 |      0 |         0 | setosa          | setosa           |    1.000 | ✓\n",
            "     6 |      1 |         1 | versicolor      | versicolor       |    0.648 | ✓\n",
            "     7 |      0 |         0 | setosa          | setosa           |    1.000 | ✓\n",
            "     8 |      0 |         0 | setosa          | setosa           |    1.000 | ✓\n",
            "\n",
            "Correct predictions in sample: 8/8\n",
            "\n",
            "Comparison with Previous Models:\n",
            "Naive Bayes accuracy:   0.9667\n",
            "Decision Tree accuracy: 0.9667\n",
            "Both Naive Bayes and Decision Tree perform equally!\n",
            "k-NN accuracy:          1.0000\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(\"Naive Bayes on Iris Dataset:\")\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Features: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "print()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "print()\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_pred_proba = nb_model.predict_proba(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Results:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "print(\"Model Parameters:\")\n",
        "print(f\"Classes: {nb_model.classes_}\")\n",
        "print(f\"Number of training samples per class: {nb_model.class_count_}\")\n",
        "print()\n",
        "\n",
        "print(\"Feature Statistics (means for each class):\")\n",
        "print(f\"{'Feature':25s}\", end=\"\")\n",
        "for class_name in iris.target_names:\n",
        "    print(f\"{class_name:>12s}\", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, feature_name in enumerate(iris.feature_names):\n",
        "    print(f\"{feature_name:25s}\", end=\"\")\n",
        "    for j, class_idx in enumerate(nb_model.classes_):\n",
        "        mean_value = nb_model.theta_[j, i]\n",
        "        print(f\"{mean_value:>12.2f}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Sample predictions (first 8 test samples):\")\n",
        "print(\"Sample | Actual | Predicted | Actual Name      | Predicted Name     | Max Prob | Match\")\n",
        "print(\"-\" * 85)\n",
        "for i in range(min(8, len(y_test))):\n",
        "    actual = y_test[i]\n",
        "    predicted = y_pred[i]\n",
        "    actual_name = iris.target_names[actual]\n",
        "    predicted_name = iris.target_names[predicted]\n",
        "    max_prob = np.max(y_pred_proba[i])\n",
        "    match = \"✓\" if actual == predicted else \"✗\"\n",
        "    print(f\"{i+1:6d} | {actual:6d} | {predicted:9d} | {actual_name:15s} | {predicted_name:15s}  | {max_prob:8.3f} | {match}\")\n",
        "\n",
        "print()\n",
        "print(f\"Correct predictions in sample: {np.sum(y_pred[:8] == y_test[:8])}/8\")\n",
        "\n",
        "print(\"\\nComparison with Previous Models:\")\n",
        "dt_accuracy = globals().get('dt_accuracy_q17')\n",
        "if dt_accuracy is not None:\n",
        "    print(f\"Naive Bayes accuracy:   {accuracy:.4f}\")\n",
        "    print(f\"Decision Tree accuracy: {dt_accuracy:.4f}\")\n",
        "    if accuracy > dt_accuracy:\n",
        "        print(\"Naive Bayes performs better than Decision Tree!\")\n",
        "    elif accuracy < dt_accuracy:\n",
        "        print(\"Decision Tree performs better than Naive Bayes!\")\n",
        "    else:\n",
        "        print(\"Both Naive Bayes and Decision Tree perform equally!\")\n",
        "\n",
        "if 'y_pred_q15' in globals() and 'y_test_q15' in globals():\n",
        "    y_test_knn = globals()['y_test_q15']\n",
        "    y_pred_knn = globals()['y_pred_q15']\n",
        "    if np.array_equal(y_test, y_test_knn):  # Same test set\n",
        "        knn_accuracy = accuracy_score(y_test_knn, y_pred_knn)\n",
        "        print(f\"k-NN accuracy:          {knn_accuracy:.4f}\")\n",
        "\n",
        "globals()['nb_model_q18'] = nb_model\n",
        "globals()['nb_accuracy_q18'] = accuracy\n",
        "globals()['X_test_q18'] = X_test\n",
        "globals()['y_test_q18'] = y_test\n",
        "globals()['y_pred_q18'] = y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "APrbUkZcKFG-",
      "metadata": {
        "id": "APrbUkZcKFG-"
      },
      "source": [
        "### Q19 — k-Means (blobs)\n",
        "Generate 3 clusters with `make_blobs(n_samples=300)`. Fit `KMeans(n_clusters=3)` and print inertia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dVyje8OdKFG-",
      "metadata": {
        "id": "dVyje8OdKFG-",
        "outputId": "ab456939-8901-4536-8a36-47d0d732343d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k-Means Clustering on Blob Dataset:\n",
            "Dataset shape: X=(300, 2)\n",
            "Number of samples: 300\n",
            "Number of features: 2\n",
            "True number of clusters: 3\n",
            "X range: [-10.59, 13.09]\n",
            "\n",
            "k-Means Results:\n",
            "Number of clusters (k): 3\n",
            "Inertia (WCSS): 1275.43\n",
            "\n",
            "Clustering Analysis:\n",
            "Cluster centers:\n",
            "  Cluster 0: [-2.70, 9.06]\n",
            "  Cluster 1: [-6.89, -7.04]\n",
            "  Cluster 2: [4.80, 2.03]\n",
            "\n",
            "Samples per cluster:\n",
            "  Cluster 0: 100 samples\n",
            "  Cluster 1: 100 samples\n",
            "  Cluster 2: 100 samples\n",
            "\n",
            "True vs Predicted Cluster Comparison:\n",
            "True clusters distribution: [100 100 100]\n",
            "Predicted clusters distribution: [100 100 100]\n",
            "\n",
            "Silhouette Score: 0.7734\n",
            "(Silhouette score ranges from -1 to 1, where 1 indicates perfect clustering)\n",
            "\n",
            "Sample cluster assignments (first 10 points):\n",
            "Point | X-coord | Y-coord | True Cluster | Predicted Cluster | Match\n",
            "----------------------------------------------------------------------\n",
            "    1 |   -7.57 |   -8.15 |            2 |                 1 |\n",
            "    2 |   -8.17 |   -7.46 |            2 |                 1 |\n",
            "    3 |   -1.28 |    7.18 |            0 |                 0 |\n",
            "    4 |    4.31 |    3.62 |            1 |                 2 |\n",
            "    5 |   -9.94 |   -8.39 |            2 |                 1 |\n",
            "    6 |    5.93 |    1.73 |            1 |                 2 |\n",
            "    7 |   -2.25 |    9.59 |            0 |                 0 |\n",
            "    8 |    5.26 |    1.13 |            1 |                 2 |\n",
            "    9 |   -3.23 |    8.74 |            0 |                 0 |\n",
            "   10 |   -4.30 |   10.00 |            0 |                 0 |\n",
            "\n",
            "Cluster Quality Metrics:\n",
            "Mean distance to cluster center: 1.831\n",
            "Max distance to cluster center: 5.756\n",
            "Min distance to cluster center: 0.154\n",
            "\n",
            "k-Means Algorithm Details:\n",
            "Number of iterations: 2\n",
            "Algorithm used: lloyd\n",
            "Initialization method: k-means++\n",
            "k-Means Results:\n",
            "Number of clusters (k): 3\n",
            "Inertia (WCSS): 1275.43\n",
            "\n",
            "Clustering Analysis:\n",
            "Cluster centers:\n",
            "  Cluster 0: [-2.70, 9.06]\n",
            "  Cluster 1: [-6.89, -7.04]\n",
            "  Cluster 2: [4.80, 2.03]\n",
            "\n",
            "Samples per cluster:\n",
            "  Cluster 0: 100 samples\n",
            "  Cluster 1: 100 samples\n",
            "  Cluster 2: 100 samples\n",
            "\n",
            "True vs Predicted Cluster Comparison:\n",
            "True clusters distribution: [100 100 100]\n",
            "Predicted clusters distribution: [100 100 100]\n",
            "\n",
            "Silhouette Score: 0.7734\n",
            "(Silhouette score ranges from -1 to 1, where 1 indicates perfect clustering)\n",
            "\n",
            "Sample cluster assignments (first 10 points):\n",
            "Point | X-coord | Y-coord | True Cluster | Predicted Cluster | Match\n",
            "----------------------------------------------------------------------\n",
            "    1 |   -7.57 |   -8.15 |            2 |                 1 |\n",
            "    2 |   -8.17 |   -7.46 |            2 |                 1 |\n",
            "    3 |   -1.28 |    7.18 |            0 |                 0 |\n",
            "    4 |    4.31 |    3.62 |            1 |                 2 |\n",
            "    5 |   -9.94 |   -8.39 |            2 |                 1 |\n",
            "    6 |    5.93 |    1.73 |            1 |                 2 |\n",
            "    7 |   -2.25 |    9.59 |            0 |                 0 |\n",
            "    8 |    5.26 |    1.13 |            1 |                 2 |\n",
            "    9 |   -3.23 |    8.74 |            0 |                 0 |\n",
            "   10 |   -4.30 |   10.00 |            0 |                 0 |\n",
            "\n",
            "Cluster Quality Metrics:\n",
            "Mean distance to cluster center: 1.831\n",
            "Max distance to cluster center: 5.756\n",
            "Min distance to cluster center: 0.154\n",
            "\n",
            "k-Means Algorithm Details:\n",
            "Number of iterations: 2\n",
            "Algorithm used: lloyd\n",
            "Initialization method: k-means++\n"
          ]
        }
      ],
      "source": [
        "X, y_true = make_blobs(\n",
        "    n_samples=300,\n",
        "    centers=3,\n",
        "    random_state=42,\n",
        "    cluster_std=1.5\n",
        ")\n",
        "\n",
        "print(\"k-Means Clustering on Blob Dataset:\")\n",
        "print(f\"Dataset shape: X={X.shape}\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"True number of clusters: 3\")\n",
        "print(f\"X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
        "print()\n",
        "\n",
        "kmeans = KMeans(\n",
        "    n_clusters=3,\n",
        "    random_state=42,\n",
        "    n_init=10,\n",
        "    max_iter=300\n",
        ")\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "inertia = kmeans.inertia_\n",
        "print(\"k-Means Results:\")\n",
        "print(f\"Number of clusters (k): {kmeans.n_clusters}\")\n",
        "print(f\"Inertia (WCSS): {inertia:.2f}\")\n",
        "print()\n",
        "\n",
        "print(\"Clustering Analysis:\")\n",
        "print(f\"Cluster centers:\")\n",
        "for i, center in enumerate(kmeans.cluster_centers_):\n",
        "    print(f\"  Cluster {i}: [{center[0]:.2f}, {center[1]:.2f}]\")\n",
        "print()\n",
        "\n",
        "unique_labels, counts = np.unique(y_pred, return_counts=True)\n",
        "print(\"Samples per cluster:\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"  Cluster {label}: {count} samples\")\n",
        "print()\n",
        "\n",
        "print(\"True vs Predicted Cluster Comparison:\")\n",
        "print(f\"True clusters distribution: {np.bincount(y_true)}\")\n",
        "print(f\"Predicted clusters distribution: {np.bincount(y_pred)}\")\n",
        "print()\n",
        "\n",
        "silhouette_avg = silhouette_score(X, y_pred)\n",
        "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "print(\"(Silhouette score ranges from -1 to 1, where 1 indicates perfect clustering)\")\n",
        "print()\n",
        "\n",
        "print(\"Sample cluster assignments (first 10 points):\")\n",
        "print(\"Point | X-coord | Y-coord | True Cluster | Predicted Cluster | Match\")\n",
        "print(\"-\" * 70)\n",
        "for i in range(min(10, len(X))):\n",
        "    x_coord = X[i, 0]\n",
        "    y_coord = X[i, 1]\n",
        "    true_cluster = y_true[i]\n",
        "    pred_cluster = y_pred[i]\n",
        "\n",
        "    print(f\"{i+1:5d} | {x_coord:7.2f} | {y_coord:7.2f} | {true_cluster:12d} | {pred_cluster:17d} |\")\n",
        "\n",
        "print()\n",
        "\n",
        "distances_to_centers = np.sqrt(((X - kmeans.cluster_centers_[y_pred])**2).sum(axis=1))\n",
        "print(\"Cluster Quality Metrics:\")\n",
        "print(f\"Mean distance to cluster center: {np.mean(distances_to_centers):.3f}\")\n",
        "print(f\"Max distance to cluster center: {np.max(distances_to_centers):.3f}\")\n",
        "print(f\"Min distance to cluster center: {np.min(distances_to_centers):.3f}\")\n",
        "\n",
        "print()\n",
        "print(\"k-Means Algorithm Details:\")\n",
        "print(f\"Number of iterations: {kmeans.n_iter_}\")\n",
        "print(f\"Algorithm used: {kmeans.algorithm}\")\n",
        "print(f\"Initialization method: {kmeans.init}\")\n",
        "\n",
        "globals()['kmeans_model_q19'] = kmeans\n",
        "globals()['X_blobs_q19'] = X\n",
        "globals()['y_true_q19'] = y_true\n",
        "globals()['y_pred_q19'] = y_pred\n",
        "globals()['inertia_q19'] = inertia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VG1-6A5zKFG-",
      "metadata": {
        "id": "VG1-6A5zKFG-"
      },
      "source": [
        "### Q20 — Simple Neural Network (MLP)\n",
        "Use `make_moons(n_samples=400, noise=0.25)`. Split 75/25, fit `MLPClassifier(hidden_layer_sizes=(8,))` and print train/test accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zq-5AwdqKFG_",
      "metadata": {
        "id": "zq-5AwdqKFG_",
        "outputId": "7475539b-dd9a-4d96-b22e-b6a75125283f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Network (MLP) on Moons Dataset:\n",
            "Dataset shape: X=(400, 2), y=(400,)\n",
            "Number of samples: 400\n",
            "Number of features: 2\n",
            "Classes: [0 1]\n",
            "Class distribution: [200 200]\n",
            "Noise level: 0.25\n",
            "\n",
            "Training set: X_train=(300, 2), y_train=(300,)\n",
            "Test set: X_test=(100, 2), y_test=(100,)\n",
            "Train class distribution: [150 150]\n",
            "Test class distribution: [50 50]\n",
            "\n",
            "Feature scaling applied (StandardScaler)\n",
            "Original X_train mean: [0.5373 0.2641]\n",
            "Scaled X_train mean: [-0.  0.]\n",
            "Scaled X_train std: [1. 1.]\n",
            "\n",
            "Multi-Layer Perceptron (MLP) Results:\n",
            "Hidden layer sizes: (8,)\n",
            "Number of layers: 3\n",
            "Number of outputs: 1\n",
            "\n",
            "Train Accuracy: 0.8867 (88.67%)\n",
            "Test Accuracy:  0.9500 (95.00%)\n",
            "\n",
            "Model Performance Analysis:\n",
            "Unusual: test accuracy higher than train accuracy (gap: -0.0633)\n",
            "\n",
            "Training Details:\n",
            "Number of iterations: 1000\n",
            "Solver: adam\n",
            "Learning rate: 0.001\n",
            "Alpha (regularization): 0.01\n",
            "Converged: False\n",
            "\n",
            "Network Architecture:\n",
            "Input layer: 2 neurons (features)\n",
            "Hidden layer 1: 8 neurons\n",
            "Output layer: 1 neurons (classes)\n",
            "Total parameters (approx): 33\n",
            "\n",
            "Sample predictions (first 8 test samples):\n",
            "Sample | Actual | Predicted | Prob(Class 0) | Prob(Class 1) | Match\n",
            "----------------------------------------------------------------------\n",
            "     1 |      1 |         1 |         0.120 |         0.880 | ✓\n",
            "     2 |      0 |         1 |         0.472 |         0.528 | ✗\n",
            "     3 |      0 |         0 |         0.822 |         0.178 | ✓\n",
            "     4 |      0 |         0 |         0.869 |         0.131 | ✓\n",
            "     5 |      0 |         0 |         0.950 |         0.050 | ✓\n",
            "     6 |      0 |         0 |         0.979 |         0.021 | ✓\n",
            "     7 |      1 |         1 |         0.051 |         0.949 | ✓\n",
            "     8 |      0 |         0 |         0.859 |         0.141 | ✓\n",
            "\n",
            "Correct predictions in sample: 7/8\n",
            "\n",
            "Comparison with Previous Models:\n",
            "MLP trained on different dataset than Logistic Regression\n",
            "Multi-Layer Perceptron (MLP) Results:\n",
            "Hidden layer sizes: (8,)\n",
            "Number of layers: 3\n",
            "Number of outputs: 1\n",
            "\n",
            "Train Accuracy: 0.8867 (88.67%)\n",
            "Test Accuracy:  0.9500 (95.00%)\n",
            "\n",
            "Model Performance Analysis:\n",
            "Unusual: test accuracy higher than train accuracy (gap: -0.0633)\n",
            "\n",
            "Training Details:\n",
            "Number of iterations: 1000\n",
            "Solver: adam\n",
            "Learning rate: 0.001\n",
            "Alpha (regularization): 0.01\n",
            "Converged: False\n",
            "\n",
            "Network Architecture:\n",
            "Input layer: 2 neurons (features)\n",
            "Hidden layer 1: 8 neurons\n",
            "Output layer: 1 neurons (classes)\n",
            "Total parameters (approx): 33\n",
            "\n",
            "Sample predictions (first 8 test samples):\n",
            "Sample | Actual | Predicted | Prob(Class 0) | Prob(Class 1) | Match\n",
            "----------------------------------------------------------------------\n",
            "     1 |      1 |         1 |         0.120 |         0.880 | ✓\n",
            "     2 |      0 |         1 |         0.472 |         0.528 | ✗\n",
            "     3 |      0 |         0 |         0.822 |         0.178 | ✓\n",
            "     4 |      0 |         0 |         0.869 |         0.131 | ✓\n",
            "     5 |      0 |         0 |         0.950 |         0.050 | ✓\n",
            "     6 |      0 |         0 |         0.979 |         0.021 | ✓\n",
            "     7 |      1 |         1 |         0.051 |         0.949 | ✓\n",
            "     8 |      0 |         0 |         0.859 |         0.141 | ✓\n",
            "\n",
            "Correct predictions in sample: 7/8\n",
            "\n",
            "Comparison with Previous Models:\n",
            "MLP trained on different dataset than Logistic Regression\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RoG\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X, y = make_moons(n_samples=400, noise=0.25, random_state=42)\n",
        "\n",
        "print(\"Neural Network (MLP) on Moons Dataset:\")\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Number of samples: {len(X)}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "print(f\"Noise level: 0.25\")\n",
        "print()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Test set: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
        "print(f\"Train class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")\n",
        "print()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Feature scaling applied (StandardScaler)\")\n",
        "print(f\"Original X_train mean: {X_train.mean(axis=0)}\")\n",
        "print(f\"Scaled X_train mean: {X_train_scaled.mean(axis=0)}\")\n",
        "print(f\"Scaled X_train std: {X_train_scaled.std(axis=0)}\")\n",
        "print()\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(8,),\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    alpha=0.01,\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.001\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_train_pred = mlp.predict(X_train_scaled)\n",
        "y_test_pred = mlp.predict(X_test_scaled)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Multi-Layer Perceptron (MLP) Results:\")\n",
        "print(f\"Hidden layer sizes: {mlp.hidden_layer_sizes}\")\n",
        "print(f\"Number of layers: {mlp.n_layers_}\")\n",
        "print(f\"Number of outputs: {mlp.n_outputs_}\")\n",
        "print()\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "print(f\"Test Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print()\n",
        "\n",
        "accuracy_diff = train_accuracy - test_accuracy\n",
        "print(\"Model Performance Analysis:\")\n",
        "if accuracy_diff > 0.1:\n",
        "    print(f\"Potential overfitting detected (train-test gap: {accuracy_diff:.4f})\")\n",
        "elif accuracy_diff < -0.05:\n",
        "    print(f\"Unusual: test accuracy higher than train accuracy (gap: {accuracy_diff:.4f})\")\n",
        "else:\n",
        "    print(f\"Good generalization (train-test gap: {accuracy_diff:.4f})\")\n",
        "print()\n",
        "\n",
        "print(\"Training Details:\")\n",
        "print(f\"Number of iterations: {mlp.n_iter_}\")\n",
        "print(f\"Solver: {mlp.solver}\")\n",
        "print(f\"Learning rate: {mlp.learning_rate_init}\")\n",
        "print(f\"Alpha (regularization): {mlp.alpha}\")\n",
        "print(f\"Converged: {mlp.n_iter_ < mlp.max_iter}\")\n",
        "print()\n",
        "\n",
        "print(\"Network Architecture:\")\n",
        "print(f\"Input layer: {X_train.shape[1]} neurons (features)\")\n",
        "print(f\"Hidden layer 1: {mlp.hidden_layer_sizes[0]} neurons\")\n",
        "print(f\"Output layer: {mlp.n_outputs_} neurons (classes)\")\n",
        "total_params = (X_train.shape[1] * mlp.hidden_layer_sizes[0] + mlp.hidden_layer_sizes[0] +\n",
        "                mlp.hidden_layer_sizes[0] * mlp.n_outputs_ + mlp.n_outputs_)\n",
        "print(f\"Total parameters (approx): {total_params}\")\n",
        "print()\n",
        "\n",
        "y_test_proba = mlp.predict_proba(X_test_scaled)\n",
        "\n",
        "print(\"Sample predictions (first 8 test samples):\")\n",
        "print(\"Sample | Actual | Predicted | Prob(Class 0) | Prob(Class 1) | Match\")\n",
        "print(\"-\" * 70)\n",
        "for i in range(min(8, len(y_test))):\n",
        "    actual = y_test[i]\n",
        "    predicted = y_test_pred[i]\n",
        "    prob_0 = y_test_proba[i][0]\n",
        "    prob_1 = y_test_proba[i][1]\n",
        "    match = \"✓\" if actual == predicted else \"✗\"\n",
        "    print(f\"{i+1:6d} | {actual:6d} | {predicted:9d} | {prob_0:13.3f} | {prob_1:13.3f} | {match}\")\n",
        "\n",
        "print()\n",
        "print(f\"Correct predictions in sample: {np.sum(y_test_pred[:8] == y_test[:8])}/8\")\n",
        "\n",
        "print(\"\\nComparison with Previous Models:\")\n",
        "if 'X_test_q14' in globals() and 'y_test_q14' in globals():\n",
        "    X_test_q14 = globals()['X_test_q14']\n",
        "    y_test_q14 = globals()['y_test_q14']\n",
        "    model_q14 = globals()['model_q14']\n",
        "\n",
        "    if X_test_q14.shape == X_test.shape:\n",
        "        logistic_pred = model_q14.predict(X_test_q14)\n",
        "        logistic_accuracy = accuracy_score(y_test_q14, logistic_pred)\n",
        "        print(f\"MLP accuracy:               {test_accuracy:.4f}\")\n",
        "        print(f\"Logistic Regression accuracy: {logistic_accuracy:.4f}\")\n",
        "        if test_accuracy > logistic_accuracy:\n",
        "            print(\"MLP performs better than Logistic Regression!\")\n",
        "        elif test_accuracy < logistic_accuracy:\n",
        "            print(\"Logistic Regression performs better than MLP!\")\n",
        "        else:\n",
        "            print(\"Both models perform equally!\")\n",
        "    else:\n",
        "        print(\"MLP trained on different dataset than Logistic Regression\")\n",
        "\n",
        "globals()['mlp_model_q20'] = mlp\n",
        "globals()['scaler_q20'] = scaler\n",
        "globals()['X_train_q20'] = X_train_scaled\n",
        "globals()['X_test_q20'] = X_test_scaled\n",
        "globals()['y_train_q20'] = y_train\n",
        "globals()['y_test_q20'] = y_test\n",
        "globals()['train_accuracy_q20'] = train_accuracy\n",
        "globals()['test_accuracy_q20'] = test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nTZx15wLKFG_",
      "metadata": {
        "id": "nTZx15wLKFG_"
      },
      "source": [
        "---\n",
        "### End of Quiz\n",
        "Save and submit your Colab notebook after completing all questions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}